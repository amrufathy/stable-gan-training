{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wgans.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3vrnVC8TMrf4",
        "rmv1F1Nqdutd",
        "agoVzcoFa9_z",
        "Ykik_FUAdgSG"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vrnVC8TMrf4",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLAWAHxk8U0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "tf.set_random_seed(123)\n",
        "np.random.seed(123)\n",
        "\n",
        "print(tf.__version__)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmv1F1Nqdutd",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yjg-HsDDohv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = (x_train - 127.5) / 127.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agoVzcoFa9_z",
        "colab_type": "text"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_DLDGqDUH7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_generator(X, batch_size=32, shuffle=True):\n",
        "  if not shuffle:\n",
        "      perm = np.array(range(len(X)))\n",
        "  else:\n",
        "      perm = np.random.permutation(len(X))\n",
        "\n",
        "  for start in range(0, len(X), batch_size):\n",
        "      end = start + batch_size if start + batch_size < len(X) else len(X)\n",
        "\n",
        "      yield X[perm[start:end]]\n",
        "\n",
        "def visualize_imgs(imgs, shape):\n",
        "  (row, col) = shape\n",
        "  (height, width) = imgs[0].shape[:2]\n",
        "  total_img = np.zeros((height * row, width * col))\n",
        "  for idx, img in enumerate(imgs):\n",
        "      i, j = idx % col, int(idx / col)\n",
        "      total_img[j*height:(j+1)*height, i*width:(i+1)*width] = img\n",
        "  return total_img\n",
        "\n",
        "def plot_img(img, title, prefix=''):\n",
        "  plt.imshow(img, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "  plt.imsave(f'figs/{prefix}{title}.png', img, cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnw7h_F2tZ_k",
        "colab_type": "text"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpqvRSxdIzyl",
        "colab_type": "text"
      },
      "source": [
        "Paper: https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ovNdGRatbe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN:\n",
        "  def __init__(self, image_size, z_dim=128, lr=1e-4):\n",
        "    self.image_size = image_size\n",
        "    self.z_dim = z_dim\n",
        "    self.lr = lr\n",
        "    self.disc_epoch_loss, self.gen_epoch_loss = [], []\n",
        "\n",
        "    self.use_bn = True\n",
        "    self.build_model()\n",
        "\n",
        "  def build_model(self):\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    self.image_real = tf.placeholder(\n",
        "        tf.float32, [None, self.image_size, self.image_size])\n",
        "    \n",
        "    self.image_fake = self.generator()\n",
        "\n",
        "    logits_real = self.discriminator(self.image_real)\n",
        "    logits_fake = self.discriminator(self.image_fake)\n",
        "\n",
        "    # discriminator loss\n",
        "    loss_real = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_real, labels=tf.ones_like(logits_real)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    loss_fake = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_fake, labels=tf.zeros_like(logits_fake)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    self.disc_loss = loss_real + loss_fake\n",
        "\n",
        "    # generator loss\n",
        "    self.gen_loss = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_fake, labels=tf.ones_like(logits_fake)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # optimizer\n",
        "    disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
        "    self.disc_update = tf.train.AdamOptimizer(self.lr, beta1=0.5).minimize(self.disc_loss, var_list=disc_vars)\n",
        "    gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
        "    self.gen_update = tf.train.AdamOptimizer(self.lr, beta1=0.5).minimize(self.gen_loss, var_list=gen_vars)\n",
        "\n",
        "    # weight clipping\n",
        "    self.weight_clip = None\n",
        "\n",
        "  def generator(self):\n",
        "    relu = tf.nn.relu\n",
        "    linear = tf.layers.dense\n",
        "\n",
        "    with tf.variable_scope('generator'):\n",
        "      self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim])\n",
        "\n",
        "      x = self.z\n",
        "      x = relu(linear(x, 1024))\n",
        "      x = relu(linear(x, 1024))\n",
        "      x = tf.nn.tanh(linear(x, self.image_size * self.image_size))\n",
        "\n",
        "      return tf.reshape(x, [-1, self.image_size, self.image_size])\n",
        "\n",
        "  def discriminator(self, input_image):\n",
        "    relu = tf.nn.relu\n",
        "    linear = tf.layers.dense\n",
        "\n",
        "    with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):\n",
        "      x = tf.reshape(input_image, [-1, self.image_size * self.image_size])\n",
        "      x = relu(linear(x, 256))\n",
        "      x = relu(linear(x, 256))\n",
        "      x = linear(x, 1)\n",
        "\n",
        "      return x\n",
        "\n",
        "  def train(self, sess, X, nepochs=50, batch_size=32, ncritic=1, nsamples=100, show_samples=True):\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    counter = 0\n",
        "    logscale = iter(np.unique(np.logspace(0, np.log10(nepochs), num=np.sqrt(nepochs).astype(np.int) + 1).astype(np.int)))\n",
        "    x = next(logscale, None)\n",
        "\n",
        "    for epoch in tqdm(range(nepochs), desc='Train loop'):\n",
        "      disc_avg_loss, gen_avg_loss = 0, 0\n",
        "\n",
        "      for batch in batch_generator(X):\n",
        "        z_batch = np.random.uniform(-1.0, 1.0, size=(batch_size, self.z_dim)).astype(np.float32)\n",
        "\n",
        "        # train discriminator\n",
        "        if (counter % (ncritic + 1)) != 0:\n",
        "          disc_loss, _ = sess.run([self.disc_loss, self.disc_update], \n",
        "                                  feed_dict={ self.image_real: batch, self.z: z_batch })\n",
        "\n",
        "          if self.weight_clip is not None:\n",
        "            sess.run(self.weight_clip)\n",
        "\n",
        "          disc_avg_loss = np.mean([disc_avg_loss, disc_loss])\n",
        "\n",
        "        # train generator\n",
        "        else:\n",
        "          gen_loss, _ = sess.run([self.gen_loss, self.gen_update], feed_dict={ self.z: z_batch })\n",
        "          gen_avg_loss = np.mean([gen_avg_loss, gen_loss])\n",
        "\n",
        "        counter += 1\n",
        "      \n",
        "      self.disc_epoch_loss.append(disc_avg_loss)\n",
        "      self.gen_epoch_loss.append(gen_avg_loss)\n",
        "\n",
        "      # sample images from generator\n",
        "      if (epoch + 1) == x and show_samples:\n",
        "        x = next(logscale, None)\n",
        "        z_sample = np.random.uniform(-1.0, 1.0, size=(nsamples, self.z_dim))\n",
        "        sample_images = sess.run(self.image_fake, feed_dict={ self.z: z_sample })\n",
        "        img = visualize_imgs(sample_images, shape=(10,10))\n",
        "        plot_img(img, f'Epoch {epoch + 1}')\n",
        "\n",
        "  def plot_losses(self):\n",
        "    plt.plot(range(len(self.disc_epoch_loss)), self.disc_epoch_loss, color = 'blue', label = 'D')\n",
        "    plt.plot(range(len(self.gen_epoch_loss)), self.gen_epoch_loss, color = 'red', label = 'G')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training loss of D & G')\n",
        "    plt.savefig('gan_losses.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "  def save(self, sess):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, f'{self.__class__.__name__}')\n",
        "\n",
        "  def load(self, sess):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, f'{self.__class__.__name__}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO2KNr1WLHhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan = GAN(image_size=28, lr=2e-4)\n",
        "\n",
        "sess = tf.Session()\n",
        "gan.train(sess, x_train, nepochs=500)\n",
        "gan.plot_losses()\n",
        "gan.save(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1rDuKw7EHuA",
        "colab_type": "text"
      },
      "source": [
        "# DCGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrZYD-ftJS4B",
        "colab_type": "text"
      },
      "source": [
        "Paper: https://arxiv.org/pdf/1511.06434.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONQUUNzSEJ5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
        "# https://www.tensorflow.org/tutorials/generative/dcgan\n",
        "\n",
        "class DCGAN(GAN):\n",
        "  def generator(self):\n",
        "    relu = tf.nn.relu\n",
        "    deconv2d = tf.layers.conv2d_transpose\n",
        "    bn = tf.layers.batch_normalization if self.use_bn else lambda x, training: x\n",
        "    linear = tf.layers.dense\n",
        "    dim = self.image_size // 4\n",
        "\n",
        "    with tf.variable_scope('generator'):\n",
        "      self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim])\n",
        "\n",
        "      x = self.z\n",
        "      x = relu(bn(linear(x, dim * dim * 128, use_bias=False), training=True))\n",
        "      x = tf.reshape(x, [-1, dim, dim, 128])\n",
        "      x = relu(bn(deconv2d(x, 128, kernel_size=5, strides=1, padding='same', use_bias=False), training=True))\n",
        "      x = relu(bn(deconv2d(x, 64, kernel_size=5, strides=2, padding='same', use_bias=False), training=True))\n",
        "      x = tf.nn.tanh(deconv2d(x, 1, kernel_size=5, strides=2, padding='same', use_bias=False))\n",
        "\n",
        "      return tf.reshape(x, [-1, self.image_size, self.image_size])\n",
        "\n",
        "  def discriminator(self, input_image):\n",
        "    lrelu = tf.nn.leaky_relu\n",
        "    conv2d = tf.layers.conv2d\n",
        "    linear = tf.layers.dense\n",
        "\n",
        "    with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):\n",
        "      x = tf.reshape(input_image, [-1, self.image_size, self.image_size, 1])\n",
        "      x = lrelu(conv2d(x, 64, kernel_size=5, strides=2, padding='same'))\n",
        "      x = lrelu(conv2d(x, 128, kernel_size=5, strides=2, padding='same'))\n",
        "      x = tf.layers.flatten(x)\n",
        "      x = linear(x, 1)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDyRqrSTazvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan = DCGAN(image_size=28, lr=2e-4)\n",
        "\n",
        "sess = tf.Session()\n",
        "gan.train(sess, x_train, nepochs=500)\n",
        "gan.plot_losses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykik_FUAdgSG",
        "colab_type": "text"
      },
      "source": [
        "# WGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVh9pHhRJbey",
        "colab_type": "text"
      },
      "source": [
        "Paper: https://arxiv.org/pdf/1701.07875.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRpoNjTA7PdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WGAN(DCGAN):\n",
        "  def build_model(self):\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    self.image_real = tf.placeholder(\n",
        "        tf.float32, [None, self.image_size, self.image_size])\n",
        "    \n",
        "    self.image_fake = self.generator()\n",
        "\n",
        "    logits_real = self.discriminator(self.image_real)\n",
        "    logits_fake = self.discriminator(self.image_fake)\n",
        "\n",
        "    # discriminator loss\n",
        "    loss_real = tf.reduce_mean(logits_real)\n",
        "    loss_fake = tf.reduce_mean(logits_fake)\n",
        "\n",
        "    self.disc_loss = loss_fake - loss_real\n",
        "\n",
        "    # generator loss\n",
        "    self.gen_loss = -tf.reduce_mean(logits_fake)\n",
        "\n",
        "    # optimizer\n",
        "    disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
        "    self.disc_update = tf.train.RMSPropOptimizer(self.lr).minimize(self.disc_loss, var_list=disc_vars)\n",
        "    gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
        "    self.gen_update = tf.train.RMSPropOptimizer(self.lr).minimize(self.gen_loss, var_list=gen_vars)\n",
        "\n",
        "    # weight clipping\n",
        "    clip_ops = []\n",
        "    for var in disc_vars:\n",
        "      clip_ops.append(var.assign(tf.clip_by_value(var, -0.01, 0.01)))\n",
        "    self.weight_clip = tf.group(*clip_ops)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_eeuIlE32TS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan = WGAN(image_size=28, lr=2e-4)\n",
        "\n",
        "sess = tf.Session()\n",
        "gan.train(sess, x_train, nepochs=500, ncritic=1)\n",
        "gan.plot_losses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StVB-bpv6u-A",
        "colab_type": "text"
      },
      "source": [
        "# WGAN &mdash; GP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj65mxbPJnMq",
        "colab_type": "text"
      },
      "source": [
        "Paper: https://arxiv.org/pdf/1704.00028.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJsfafAk7RgF",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class WGANGP(DCGAN):\n",
        "  def __init__(self, image_size, z_dim=128, lr=1e-4):\n",
        "    super().__init__(image_size=image_size, z_dim=z_dim, lr=lr)\n",
        "\n",
        "    self.use_bn = False\n",
        "    self.build_model()\n",
        "\n",
        "  def build_model(self):\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    self.image_real = tf.placeholder(\n",
        "        tf.float32, [None, self.image_size, self.image_size])\n",
        "    \n",
        "    self.image_fake = self.generator()\n",
        "\n",
        "    logits_real = self.discriminator(self.image_real)\n",
        "    logits_fake = self.discriminator(self.image_fake)\n",
        "\n",
        "    # gradient penalty\n",
        "    batch_size = 32\n",
        "    alpha = tf.random_uniform([batch_size, self.image_size, self.image_size], 0.0, 1.0)\n",
        "    distribution = alpha * self.image_real + (1 - alpha) * self.image_fake\n",
        "    # add `1e-18` to prevent zero-norm vector (https://github.com/tensorflow/tensorflow/issues/12071)\n",
        "    gradients = tf.gradients(tf.nn.sigmoid(self.discriminator(distribution)) + 1e-18, [distribution])[0] + 1e-18\n",
        "    grad_pen = 10 * tf.reduce_mean(tf.square(tf.norm(gradients) - 1.0))\n",
        "\n",
        "    # discriminator loss\n",
        "    loss_real = tf.reduce_mean(logits_real)\n",
        "    loss_fake = tf.reduce_mean(logits_fake)\n",
        "\n",
        "    self.disc_loss = loss_fake - loss_real + grad_pen\n",
        "\n",
        "    # generator loss\n",
        "    self.gen_loss = -tf.reduce_mean(logits_fake)\n",
        "\n",
        "    # optimizer\n",
        "    disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
        "    self.disc_update = tf.train.AdamOptimizer(self.lr, beta1=0.5, beta2=0.9).minimize(self.disc_loss, var_list=disc_vars)\n",
        "    gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
        "    self.gen_update = tf.train.AdamOptimizer(self.lr, beta1=0.5, beta2=0.9).minimize(self.gen_loss, var_list=gen_vars)\n",
        "\n",
        "    # weight clipping\n",
        "    self.weight_clip = None\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X984oO8UJ9HT",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "gan = WGANGP(image_size=28, lr=2e-4)\n",
        "\n",
        "sess = tf.Session()\n",
        "gan.train(sess, x_train, nepochs=500)\n",
        "gan.plot_losses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79fA4VWr5tK7",
        "colab_type": "text"
      },
      "source": [
        "# WGAN &mdash; CT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhpX7C_e5xGP",
        "colab_type": "text"
      },
      "source": [
        "Paper: https://arxiv.org/pdf/1803.01541.pdf"
      ]
    }
  ]
}